{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordPredict.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPoride/7yO5f0NRhLjLvpI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maruf-S/WordEmbeddingsWithLTSM/blob/main/WordPredict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH4Fwo1ScODG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8A3eK6wlcTw",
        "outputId": "1ce8aef6-9fc7-462d-8df2-ebf9da21c33d"
      },
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "IdRcVIhtRGlF",
        "outputId": "5777a2a9-6a4e-4aa0-c224-1180d344269a"
      },
      "source": [
        "#Load the text files in a plain text format to train the mdel\n",
        "from google.colab import files\n",
        "path_to_file = list(files.upload().keys())[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d2cc6981-faa5-4f40-9840-c7a1547b8a62\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d2cc6981-faa5-4f40-9840-c7a1547b8a62\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Doc1.txt to Doc1 (1).txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7qCavz3mKjf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWQLg24bpyJn",
        "outputId": "f5b7d854-f7d7-4baa-8be7-55ec39224e21"
      },
      "source": [
        "#Vocab\n",
        "# example = text.replace(\"\\n\", \" \")\n",
        "import string\n",
        "##\n",
        "example = text\n",
        "# print(example[:100])\n",
        "##\n",
        "\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# example = word_tokenize(example)\n",
        "print(example[:15])\n",
        "# example = example.lower()\n",
        "# example = example.split()\n",
        "# print(example[:100])\n",
        "# import string\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "example = [w.translate(table) for w in example]\n",
        "vocab = sorted(set(example))\n",
        "print(len(vocab))\n",
        "print(vocab[:100])\n",
        "voc_size = len(vocab)\n",
        "len(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r\n",
            "\t\r\n",
            "\r\n",
            "badlands\n",
            "43\n",
            "['', '\\t', '\\n', '\\r', ' ', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '“', '”']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1077841"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M9Yoolw_Acp"
      },
      "source": [
        "# def text_to_int(text):\n",
        "#   return np.array([char2idx[c] for c in text])\n",
        "\n",
        "# text_as_int = text_to_int(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyIX2n4IAMwT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7AZNI7aRz6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aa2961f-f1bc-4a1b-8cba-82e7e49be7f0"
      },
      "source": [
        "# Creating a mapping from words characters to ints\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "def word_to_int(text):\n",
        "  return np.array([char2idx[c] for c in example])\n",
        "\n",
        "word_as_int = word_to_int(text)\n",
        "# print(type(list(word_as_int[:10])))\n",
        "print(\"////////////////////////\")\n",
        "# print(word_to_int(\"\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "////////////////////////\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2sj8AOSvcoX"
      },
      "source": [
        "# onehot_repr=[one_hot(words,voc_size)for words in example] \n",
        "# arr = np.array(onehot_repr)\n",
        "# print(arr)\n",
        "# # onehot_repr = [ int(x) for x in onehot_repr ]#get rid of the sublist\n",
        "# # print(onehot_repr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0xEnZjJwVjH",
        "outputId": "6f224a61-39d7-4df7-b8be-1504090b4190"
      },
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(word_as_int[:13]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "badlandsbyterencemalickfinalversiondialogueandcontinuityfadeinint.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lIHmk1t2FWT"
      },
      "source": [
        "# model = tf.keras.Sequential([\n",
        "#     tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
        "#     tf.keras.layers.LSTM(32),\n",
        "#     tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "# ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GweykVYm2o_m"
      },
      "source": [
        "seq_length = 10 # length of sequence for a training example\n",
        "examples_per_epoch = len(example)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(word_as_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5ehMHldBBOi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqmxfT7gVGlr"
      },
      "source": [
        "Next we can use the batch method to turn this stream of characters into batches of desired length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi0xaPB_VOJl"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03zKVHTvV0Km"
      },
      "source": [
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p_y2YmgWbnc",
        "outputId": "f268af2e-3331-4fed-bc86-b60f66a469e5"
      },
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "badlandsbyterencemalickfinalversiondialogueandcontinuityfade\n",
            "\n",
            "OUTPUT\n",
            "byterencemalickfinalversiondialogueandcontinuityfadein\n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "int.bedroomthetimeis1959.hollysargis,\n",
            "\n",
            "OUTPUT\n",
            ".bedroomthetimeis1959.hollysargis,the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRsKcjhXXuoD"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "voc_size = voc_size  # vocab is number of unique words\n",
        "EMBEDDING_DIM = 30\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v_P2dEic4qt",
        "outputId": "057e42cc-4060-493d-f686-6a7ce55defef"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "model = build_model(voc_size,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (64, None, 30)            312750    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (64, None, 1024)          4321280   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (64, None, 10425)         10685625  \n",
            "=================================================================\n",
            "Total params: 15,319,655\n",
            "Trainable params: 15,319,655\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdvEqlwc6_q0",
        "outputId": "0f6e5fc1-e276-452a-aeaa-e4a5ea95c1b3"
      },
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 10, 10425) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQS5KXwi7_NX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4964437f-0199-4a6d-9088-fbecc1bf76eb"
      },
      "source": [
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-3.95137176e-05  9.44193627e-04 -1.74029003e-04 ... -7.21865566e-04\n",
            "    5.24793686e-05  4.57966089e-04]\n",
            "  [ 3.60347214e-04  8.56439292e-05  3.78956756e-04 ... -3.62775434e-04\n",
            "   -3.26882000e-04  4.33132955e-04]\n",
            "  [ 7.44409626e-04 -2.20457659e-04 -2.40832567e-04 ...  3.50860966e-04\n",
            "   -3.60447943e-04  2.82003777e-04]\n",
            "  ...\n",
            "  [ 4.64327559e-06  3.80402314e-04  1.90953258e-04 ... -5.66780451e-04\n",
            "    1.24066602e-04  1.03855855e-03]\n",
            "  [ 9.92214609e-06 -4.16419614e-04  8.02315539e-04 ...  1.22168685e-05\n",
            "    7.77290377e-04  6.39292470e-04]\n",
            "  [ 4.44218138e-04  6.41159859e-05  1.05696975e-03 ...  4.45553560e-05\n",
            "    8.14540486e-04  9.24728636e-04]]\n",
            "\n",
            " [[-4.60212352e-04  5.70160395e-04 -3.79910925e-04 ... -2.45814357e-04\n",
            "   -3.39056656e-04 -7.61182018e-05]\n",
            "  [-4.40537522e-04  5.21148031e-04 -6.25294750e-04 ... -2.27550714e-04\n",
            "    1.71605061e-05 -4.03533544e-04]\n",
            "  [-3.44105822e-04  1.15747878e-03 -7.31018954e-04 ... -8.32607679e-04\n",
            "    5.89771844e-05  2.47059477e-04]\n",
            "  ...\n",
            "  [-3.61099868e-04  4.31159278e-04 -5.79311047e-04 ... -8.80638312e-04\n",
            "   -3.59816826e-04  2.47913500e-04]\n",
            "  [ 3.37484904e-04  6.31814590e-04 -1.04965130e-03 ... -9.34578784e-05\n",
            "    4.83437267e-04  2.00861941e-05]\n",
            "  [-6.07175280e-05  8.51878372e-04 -5.75703045e-04 ...  2.42028327e-05\n",
            "   -1.22550890e-04  3.27612041e-04]]\n",
            "\n",
            " [[ 4.66164383e-05 -1.22487079e-04 -4.01181023e-04 ... -3.55129479e-04\n",
            "   -8.50617420e-04  7.90412287e-06]\n",
            "  [-2.53061680e-05  8.82266380e-04 -4.51144500e-04 ... -9.55463853e-04\n",
            "   -4.41663782e-04  4.75097215e-04]\n",
            "  [ 3.34144657e-04  8.40580731e-04 -1.92157138e-04 ...  6.49387439e-05\n",
            "    2.64224538e-04  8.33884755e-04]\n",
            "  ...\n",
            "  [ 5.84653637e-04 -4.10951034e-04  1.00620629e-04 ...  6.15561323e-04\n",
            "    4.30497894e-04  6.58802746e-04]\n",
            "  [-1.20987439e-04  2.80812004e-04 -1.85469078e-04 ...  3.77041142e-04\n",
            "    1.49975392e-06  4.85111115e-04]\n",
            "  [-4.47700848e-04 -1.38764153e-04  3.61239916e-04 ...  3.02771659e-04\n",
            "   -5.38595777e-04  5.62560395e-04]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 3.68279609e-04 -3.54217627e-04  7.83230353e-05 ... -7.37322480e-05\n",
            "   -2.66040937e-04 -9.27638757e-05]\n",
            "  [ 3.49875860e-04 -4.22564830e-04 -4.35465336e-04 ...  4.10993489e-05\n",
            "   -3.48911708e-04 -2.78862717e-04]\n",
            "  [ 6.87213847e-04  1.70340092e-04 -1.04436322e-05 ... -5.22619579e-04\n",
            "   -4.31479770e-04 -1.23603910e-04]\n",
            "  ...\n",
            "  [ 4.18617943e-04 -2.00420051e-04  5.31386504e-05 ...  5.49850112e-04\n",
            "    1.90377221e-04  3.22141685e-04]\n",
            "  [ 7.07728963e-04  3.68410867e-04  3.76572745e-04 ... -1.89166181e-04\n",
            "   -9.51556285e-05  1.55355810e-04]\n",
            "  [ 2.65524490e-04 -3.29751929e-05  1.81540265e-04 ...  2.24501957e-04\n",
            "   -5.19056513e-04 -5.12950763e-04]]\n",
            "\n",
            " [[ 3.40505794e-04 -2.08953556e-04  8.36625986e-04 ...  2.62357178e-04\n",
            "    4.51675704e-04  2.41393573e-04]\n",
            "  [ 1.75876121e-04  3.26885609e-04  6.10184623e-04 ... -3.17550189e-06\n",
            "    3.52680858e-04 -3.91731155e-04]\n",
            "  [ 3.68197565e-04 -1.32657195e-04  5.63727517e-04 ...  3.49523674e-04\n",
            "    1.18242722e-04 -7.15404094e-05]\n",
            "  ...\n",
            "  [ 3.26249894e-04  7.64624041e-04 -6.40238402e-04 ... -1.53683781e-04\n",
            "   -7.95504136e-04 -3.70997906e-04]\n",
            "  [-3.81855003e-04  1.43230049e-04 -7.78142021e-06 ... -3.06454225e-04\n",
            "   -1.04569260e-03 -1.20530873e-04]\n",
            "  [-5.59057167e-04  4.05911822e-04 -7.52359629e-05 ... -8.48082826e-04\n",
            "   -3.44190456e-04  6.24390595e-05]]\n",
            "\n",
            " [[-2.18654386e-04 -1.50231077e-04  2.32920487e-04 ...  1.61458738e-04\n",
            "   -7.16480572e-05  4.89796912e-05]\n",
            "  [ 8.17887048e-05 -1.13759430e-04 -2.44704890e-04 ...  3.82391707e-04\n",
            "    4.38046118e-05  1.43044861e-04]\n",
            "  [ 4.75102242e-05  8.63551802e-04 -3.40392377e-04 ... -5.29857585e-04\n",
            "    1.56538532e-04  5.02516690e-04]\n",
            "  ...\n",
            "  [-1.04845916e-04  1.34760328e-03 -2.09886493e-04 ... -9.92437126e-04\n",
            "    4.28875501e-04  5.45405259e-04]\n",
            "  [-2.82372901e-04  1.61603861e-03  2.35403873e-04 ... -5.11985098e-04\n",
            "    6.77383330e-04  1.07402680e-03]\n",
            "  [ 3.88416491e-04  1.69546693e-03  1.45613187e-04 ...  1.79054565e-04\n",
            "    1.08473795e-03  8.71502096e-04]]], shape=(64, 10, 10425), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA1Zhop28V9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1edc5a46-83e9-49bc-f2a3-7331867d9a00"
      },
      "source": [
        "# lets examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n",
            "tf.Tensor(\n",
            "[[-3.95137176e-05  9.44193627e-04 -1.74029003e-04 ... -7.21865566e-04\n",
            "   5.24793686e-05  4.57966089e-04]\n",
            " [ 3.60347214e-04  8.56439292e-05  3.78956756e-04 ... -3.62775434e-04\n",
            "  -3.26882000e-04  4.33132955e-04]\n",
            " [ 7.44409626e-04 -2.20457659e-04 -2.40832567e-04 ...  3.50860966e-04\n",
            "  -3.60447943e-04  2.82003777e-04]\n",
            " ...\n",
            " [ 4.64327559e-06  3.80402314e-04  1.90953258e-04 ... -5.66780451e-04\n",
            "   1.24066602e-04  1.03855855e-03]\n",
            " [ 9.92214609e-06 -4.16419614e-04  8.02315539e-04 ...  1.22168685e-05\n",
            "   7.77290377e-04  6.39292470e-04]\n",
            " [ 4.44218138e-04  6.41159859e-05  1.05696975e-03 ...  4.45553560e-05\n",
            "   8.14540486e-04  9.24728636e-04]], shape=(10, 10425), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbIoe7Ei8q3q",
        "outputId": "d6a9075f-b87a-497c-84ef-5fc5a7acb554"
      },
      "source": [
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 65 values representing the probabillity of each character occuring next"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10425\n",
            "tf.Tensor(\n",
            "[-3.9513718e-05  9.4419363e-04 -1.7402900e-04 ... -7.2186557e-04\n",
            "  5.2479369e-05  4.5796609e-04], shape=(10425,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlEYM1H995gR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "7be83a65-36f4-45d7-873f-ae713bb744b4"
      },
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'tillwonderprojectsmammockeddictatorhatchceremoniousincurministerprosecute'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOw23fWq9D9O"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g6o7zA_hAiS"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7aMushYjSpy"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4PAgrwMjZ4_"
      },
      "source": [
        "history = model.fit(data, epochs=100, callbacks=[checkpoint_callback])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPSto3uimSKp"
      },
      "source": [
        "model = build_model(voc_size, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZIEZWE4mNKl"
      },
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEpKH9zCL2hI",
        "outputId": "25c48aeb-6f22-4b5a-ee95-a416a5df1355"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (1, None, 30)             312750    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (1, None, 1024)           4321280   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (1, None, 10425)          10685625  \n",
            "=================================================================\n",
            "Total params: 15,319,655\n",
            "Trainable params: 15,319,655\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPSALdQXA3l3"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of words to generate\n",
        "  num_generate = 30\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  # input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = [char2idx[s] for s in start_string.split()] \n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "    \n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + \" \" + ' '.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAJqhD9AA5mF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1429500c-b5ce-41d4-b945-b10f233914ee"
      },
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type a starting string: color\n",
            "color santa explains them , gentle cominius : do not hear , we 'll muster in my feet . but tell me not me to say . guess you 're on\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}